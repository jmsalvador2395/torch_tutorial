{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SVM Classifier in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the Data\n",
    "\n",
    "same procedure as the the numpy version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "\t\tds_name='mnist', \n",
    "\t\tdata_dir='./datasets', \n",
    "\t\tbatch_size=4096, \n",
    "\t\tget_mean=False):\n",
    "\t\"\"\"\n",
    "\tdownloads the dataset and returns a dataloader for batching\n",
    "\n",
    "\t:param ds_name: name of the dataset. check huggingface for valid \n",
    "\t\t\t\t\tdataset names\n",
    "\t:param data_dir: the directory to download and store the data\n",
    "\t:param batch_size: the batch size for the classifier to train on\n",
    "\n",
    "\t:return: two pytorch dataloaders for the training and testing data\n",
    "\t\"\"\"\n",
    "\t# create path for dataset if it doesn't exist\n",
    "\tPath(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\t# download or read in dataset\n",
    "\tds = datasets.load_dataset(\n",
    "            ds_name,\n",
    "            cache_dir=data_dir,\n",
    "\t\t\tnum_proc=12,\n",
    "            keep_in_memory=True).with_format('pt')\n",
    "\n",
    "\t# zero mean procedure. only computes mean based of training set\n",
    "\tif get_mean:\n",
    "\t\tnew_ds = {\n",
    "\t\t\tsplit : {'image' : [], 'label' : []} \n",
    "\t\t\tfor split in ds.keys()\n",
    "\t\t}\n",
    "\t\tmean_x = torch.mean(torch.tensor(\n",
    "\t\t\t[sample['image'] for sample in ds['train']]\n",
    "\t\t), dtype=np.float32)\n",
    "\n",
    "\t\treturn  (\n",
    "\t\t\tds,\n",
    "\t\t\tmean_x,\n",
    "\t\t\tDataLoader(ds['train'], batch_size=batch_size),\n",
    "\t\t\tDataLoader(ds['test'], batch_size=batch_size),\n",
    "\t\t)\n",
    "\treturn  (\n",
    "\t\tds,\n",
    "\t\tDataLoader(ds['train'], batch_size=batch_size),\n",
    "\t\tDataLoader(ds['test'], batch_size=batch_size),\n",
    "\t)\n",
    "\n",
    "ds, train_dl, test_dl = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SVM class\n",
    "\n",
    "Here we define the architecture, loss and gradient calculation, and\n",
    "optimization steps for the classifier.\n",
    "\n",
    "In pytorch, regularization is taken care of by the optimizer so instead\n",
    "we train on the loss function\n",
    "\n",
    "Notice that unlike the numpy version, there is no explicit computation\n",
    "of the gradient in any of our code.\n",
    "For linear classifiers like the SVM, this doesn't really save that much\n",
    "time, but for larger models like BERT, the amount of gradient\n",
    "computations you need to do quickly get out of hand.\n",
    "\n",
    "$$\n",
    "\\mathcal{L_\\theta(x)}=\\frac{1}{N}\\sum\\limits_{i=1}^N \\max(0, 1-y_if_\\theta(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(nn.Module):\n",
    "    \"\"\" implementation of the SVM classifier \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            C=10, dim=784, shape=(28, 28), \n",
    "            reg=1, lr=1, loc=0, scale=1, \n",
    "            decay=1, mean_x=None):\n",
    "        \"\"\"\n",
    "        initialize the weights\n",
    "\n",
    "        :param C: number of classes\n",
    "        :param dim: dimensionality of the input\n",
    "        :param reg: the regularization term\n",
    "        :param loc: arguement for initialization using \n",
    "                    numpy.random.normal()\n",
    "        :param scale: standard deviation for initialization using \n",
    "                      numpy.random.normal()\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # save dimensionality for reshaping input\n",
    "        self.dim=dim\n",
    "\n",
    "        # initialize weights. use +1 for the bias weights\n",
    "        self.w = nn.Linear(dim+1, C)\n",
    "\n",
    "        # set bias weights to 0\n",
    "        self.w.weight.data[-1, :] = 0\n",
    "\n",
    "        # sets the mean of the data\n",
    "        self.mean_x = mean_x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        generates classifier scores. computes x@w+b\n",
    "\n",
    "        :param x: the input data\n",
    "\n",
    "        :return: prediction scores\n",
    "        \"\"\"\n",
    "\n",
    "        # get number of samples\n",
    "        N=x.shape[0]\n",
    "        if self.mean_x:\n",
    "            N-=self.mean_x\n",
    "\n",
    "        # reshape x to an Nxdim matrix and then pad with 1s\n",
    "        xpad=torch.hstack((\n",
    "            x.reshape(N, self.dim),\n",
    "            torch.ones(N, device=x.device)[:, None]\n",
    "        ))\n",
    "\n",
    "        return self.w(xpad)\n",
    "\n",
    "    def loss(self, x, y):\n",
    "        \"\"\"\n",
    "        this is used for calculating the loss function\n",
    "        NOTE: Notice there is no need to explicitly compute the gradient\n",
    "        \n",
    "        :param x: the input data\n",
    "        :param y: the labels for the input data\n",
    "\n",
    "        :return loss: the output of the loss function\n",
    "        :return acc: the accuracy of the model\n",
    "        \"\"\"\n",
    "\n",
    "        # get the number of samples\n",
    "        N=x.shape[0]\n",
    "\n",
    "        # use this to index the scores\n",
    "        xi=range(N)\n",
    "\n",
    "        # calculate scores\n",
    "        scores=self(x)\n",
    "\n",
    "        preds=torch.argmax(scores, axis=-1)\n",
    "\n",
    "        # compute accuracy\n",
    "        acc=torch.sum(preds==y)/len(preds)\n",
    "\n",
    "        # compute loss\n",
    "        ys = -torch.ones(scores.shape, device=scores.device)\n",
    "        ys[xi, y] = 1\n",
    "        svm_scores=torch.maximum(torch.tensor(0), 1-ys*scores)\n",
    "        loss=torch.mean(svm_scores)\n",
    "\n",
    "        return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimization Parameters and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccad87b1f1d486caa0709949ec07b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# parameters\n",
    "lr = .01\n",
    "weight_decay = 1\n",
    "gamma = .995\n",
    "num_epochs = 40\n",
    "\n",
    "# define model and load into GPU\n",
    "model = SVM().to('cuda')\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    #weight_decay=weight_decay\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "    optimizer,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "# define evaluation procedure\n",
    "def evaluate(model, data):\n",
    "\n",
    "    # set model to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    N=len(data)\n",
    "    loss=0\n",
    "    acc=0\n",
    "    # compute loss and accuracy on the test set\n",
    "    for batch in data:\n",
    "\n",
    "        # split into data and labels. \n",
    "        x = batch['image'].to('cuda')\n",
    "        y = batch['label'].to('cuda')\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            loss_i, acc_i = model.loss(x, y)\n",
    "\n",
    "        loss+=loss_i.cpu().numpy()\n",
    "        acc+=acc_i.cpu().numpy()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    return loss/N, acc/N\n",
    "\n",
    "\n",
    "# Training loop\n",
    "history={\n",
    "    'train_loss':[],\n",
    "    'train_acc':[],\n",
    "    'test_loss':[],\n",
    "    'test_acc':[],\n",
    "    'w':[],\n",
    "}\n",
    "bar=tqdm(range(num_epochs*len(train_dl)))\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in train_dl:\n",
    "        bar.update()\n",
    "\n",
    "        # split into data and labels. \n",
    "        data, labels = batch['image'], batch['label']\n",
    "        N = data.shape[0]\n",
    "\n",
    "        loss, acc = model.loss(\n",
    "            data.to('cuda'), \n",
    "            labels.to('cuda'),\n",
    "        )\n",
    "\n",
    "        # compute backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # collect historical data\n",
    "        history['train_loss'].append(loss.detach().cpu().numpy())\n",
    "        history['train_acc'].append(acc.detach().cpu().numpy())\n",
    "\n",
    "        # collect historical data\n",
    "        test_loss, test_acc = evaluate(model, test_dl)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['w'].append(\n",
    "            model.w.weight.data.detach().cpu().numpy()\n",
    "        )\n",
    "            \n",
    "        # update progress bar\n",
    "        bar.set_postfix({\n",
    "            'loss_trn': history['train_loss'][-1],\n",
    "            'loss_tst': history['test_loss'][-1],\n",
    "            'acc_trn': history['train_acc'][-1],\n",
    "            'acc_tst': history['test_acc'][-1],\n",
    "            'lr': scheduler.get_last_lr(),\n",
    "            '||w||':np.linalg.norm(history['w'][-1], ord=2),\n",
    "        })\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
