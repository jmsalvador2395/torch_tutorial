{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The SVM Classifier\n",
    "\n",
    "In this notebook we'll run through an implementation of the \n",
    "[SVM classifier](https://www.robots.ox.ac.uk/~az/lectures/ml/lect2.pdf) \n",
    "in numpy.\n",
    "\n",
    "In the next notebook, we will implement it in pytorch instead and\n",
    "showcase the simplicity of working with it over numpy.\n",
    "\n",
    "Most of this code is taken from \n",
    "[my other repository](https://github.com/jmsalvador2395/svm/tree/master)\n",
    "which shows a lot more visualizations of the weights and \n",
    "training results so if you're interested you can go there for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datasets\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from numpy.linalg import norm\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the Data\n",
    "\n",
    "To do this we'll use the `datasets` library from huggingface.\n",
    "Don't worry about this for now since this will be covered in later\n",
    "tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(\n",
    "\t\tds_name='mnist', \n",
    "\t\tdata_dir='./datasets', \n",
    "\t\tbatch_size=16, \n",
    "\t\tget_mean=False):\n",
    "\t\"\"\"\n",
    "\tdownloads the dataset and returns a dataloader for batching\n",
    "\n",
    "\t:param ds_name: name of the dataset. check huggingface for valid \n",
    "\t\t\t\t\tdataset names\n",
    "\t:param data_dir: the directory to download and store the data\n",
    "\t:param batch_size: the batch size for the classifier to train on\n",
    "\n",
    "\t:return: two pytorch dataloaders for the training and testing data\n",
    "\t\"\"\"\n",
    "\t# create path for dataset if it doesn't exist\n",
    "\tPath(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\t# download or read in dataset\n",
    "\tds = datasets.load_dataset(\n",
    "            ds_name,\n",
    "            cache_dir = data_dir,\n",
    "            keep_in_memory=True).with_format('numpy')\n",
    "\n",
    "\t# zero mean procedure. only computes mean based of training set\n",
    "\tif get_mean:\n",
    "\t\tnew_ds = {\n",
    "\t\t\tsplit : {'image' : [], 'label' : []} \n",
    "\t\t\tfor split in ds.keys()\n",
    "\t\t}\n",
    "\t\tmean_x = np.mean(np.array(\n",
    "\t\t\t[sample['image'] for sample in ds['train']]\n",
    "\t\t), dtype=np.float32)\n",
    "\n",
    "\t\treturn  (\n",
    "\t\t\tds,\n",
    "\t\t\tmean_x,\n",
    "\t\t\tDataLoader(ds['train'], batch_size=batch_size),\n",
    "\t\t\tDataLoader(ds['test'], batch_size=batch_size),\n",
    "\t\t)\n",
    "\treturn  (\n",
    "\t\tds,\n",
    "\t\tDataLoader(ds['train'], batch_size=batch_size),\n",
    "\t\tDataLoader(ds['test'], batch_size=batch_size),\n",
    "\t)\n",
    "\n",
    "ds, train_dl, test_dl = get_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the SVM class\n",
    "\n",
    "Here we define the architecture, loss and gradient calculation, and\n",
    "optimization steps for the classifier.\n",
    "\n",
    "We train our model on the loss function\n",
    "$$\n",
    "\\mathcal{L_\\theta(x)}=\\lambda R_2(\\theta)+\\frac{1}{N}\\sum\\limits_{i=1}^N \\max(0, 1-y_if_\\theta(x_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    \"\"\" implementation of the SVM classifier \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            C=10, dim=784, shape=(28, 28), \n",
    "            reg=1, lr=1, loc=0, scale=1, \n",
    "            decay=1, mean_x=None):\n",
    "        \"\"\"\n",
    "        initialize the weights\n",
    "\n",
    "        :param C: number of classes\n",
    "        :param dim: dimensionality of the input\n",
    "        :param reg: the regularization term\n",
    "        :param loc: arguement for initialization using \n",
    "                    numpy.random.normal()\n",
    "        :param scale: standard deviation for initialization using \n",
    "                      numpy.random.normal()\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        # save the learning rate\n",
    "        self.lr=lr\n",
    "        self.lr_decayed=lr\n",
    "\n",
    "        # save the regularization term\n",
    "        self.reg=reg\n",
    "\n",
    "        # save dimensionality for reshaping input\n",
    "        self.dim=dim\n",
    "\n",
    "        # save the shape of the data\n",
    "        self.shape=shape\n",
    "\n",
    "        # save the learning rate decay\n",
    "        self.decay=decay\n",
    "\n",
    "        # initialize weights. use +1 for the bias weights\n",
    "        self.w=np.random.normal(loc=loc, scale=scale, size=(dim+1, C))\n",
    "\n",
    "        # set bias weights to 0\n",
    "        self.w[-1, :]=0\n",
    "\n",
    "        # sets the mean of the data\n",
    "        self.mean_x = mean_x\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        generates classifier scores. computes x@w+b\n",
    "\n",
    "        :param x: the input data\n",
    "\n",
    "        :return: prediction scores\n",
    "        \"\"\"\n",
    "\n",
    "        # get number of samples\n",
    "        N=x.shape[0]\n",
    "        if self.mean_x:\n",
    "            N-=self.mean_x\n",
    "\n",
    "        # reshape x to an Nxdim matrix and then pad with 1s\n",
    "        xpad=np.hstack((\n",
    "            x.reshape(N, self.dim),\n",
    "            np.ones(N)[:, None]\n",
    "        ))\n",
    "\n",
    "        return xpad@self.w\n",
    "\n",
    "    def loss(self, x, y=None):\n",
    "        \"\"\"\n",
    "        this is used for calculating the loss function\n",
    "        \n",
    "        :param x: the input data\n",
    "        :param y: the labels for the input data\n",
    "\n",
    "        :return loss: the output of the loss function\n",
    "        :return grad: the weight gradient\n",
    "        :return acc: the accuracy of the model\n",
    "        \"\"\"\n",
    "\n",
    "        # get the number of samples\n",
    "        N=x.shape[0]\n",
    "\n",
    "        # reshape x to an Nxdim matrix and then pad with 1s\n",
    "        xpad=np.hstack((\n",
    "            x.reshape(N, self.dim),\n",
    "            np.ones(N)[:, None]\n",
    "        ))\n",
    "\n",
    "        # use this to index the scores\n",
    "        xi=range(N)\n",
    "\n",
    "        # calculate scores\n",
    "        scores=self(x)\n",
    "\n",
    "        preds=np.argmax(scores, axis=-1)\n",
    "\n",
    "        # compute accuracy\n",
    "        acc=np.mean(preds==y)\n",
    "        #acc=np.sum(preds==y)\n",
    "\n",
    "        # compute loss\n",
    "        ys=-np.ones(scores.shape)\n",
    "        ys[xi, y]=1\n",
    "        svm_scores=np.maximum(0, 1-ys*scores)\n",
    "        loss=np.mean(svm_scores)+self.reg*norm(self.w, ord=2)\n",
    "\n",
    "        # compute gradient\n",
    "        mask=(svm_scores==0)\n",
    "        grad=-ys\n",
    "        grad[mask]=0\n",
    "        grad=xpad.T@grad/grad.size+self.reg*2*self.w\n",
    "\n",
    "        return loss, grad, acc\n",
    "    \n",
    "    def optim_step(self, grad):\n",
    "        \"\"\"\n",
    "        use this for the optimization step. Implementation is SGD\n",
    "\n",
    "        :param grad: the gradient of the current training step\n",
    "        \"\"\"\n",
    "        self.w-=self.lr_decayed*grad\n",
    "        self.lr_decayed = max(self.lr_decayed*self.decay, 1e-3)\n",
    "\n",
    "    def w_norm(self, ord=2):\n",
    "        \"\"\"return the sum of norms of the weight vectors\"\"\"\n",
    "        return np.linalg.norm(self.w, ord=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Training Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data):\n",
    "    N=len(data)\n",
    "    loss=0\n",
    "    acc=0\n",
    "    # compute loss and accuracy on the test set\n",
    "    for batch in data:\n",
    "\n",
    "        # split into data and labels. \n",
    "        x, y = (batch['image'].numpy(), batch['label'].numpy())\n",
    "\n",
    "        # compute loss\n",
    "        loss_i, _, acc_i = model.loss(x, y)\n",
    "\n",
    "        loss+=loss_i\n",
    "        acc+=acc_i\n",
    "    \n",
    "    return loss/N, acc/N\n",
    "\n",
    "def train(\n",
    "        model=None, C=10, \n",
    "        dim=784, lr=.01, \n",
    "        decay=.995, reg=1, \n",
    "        batch_size=4096, \n",
    "        num_epochs=15, mean_x=None):\n",
    "    \"\"\" instantiate and train an SVM classifier \"\"\"\n",
    "\n",
    "    # instantiate model\n",
    "    model = SVM(\n",
    "        C,\n",
    "        dim,\n",
    "        lr=lr,\n",
    "        decay=decay,\n",
    "        reg=reg,\n",
    "        mean_x=mean_x,\n",
    "    )\n",
    "\n",
    "    # get dataset\n",
    "    _, train_data, test_data = get_dataloaders(batch_size=batch_size)\n",
    "\n",
    "    history={\n",
    "        'train_loss':[],\n",
    "        'train_acc':[],\n",
    "        'test_loss':[],\n",
    "        'test_acc':[],\n",
    "        'w':[],\n",
    "    }\n",
    "\n",
    "    bar=tqdm(range(num_epochs*len(train_data)))\n",
    "    import time\n",
    "    \n",
    "    # outer training loop is for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # inner loop iterates through the whole dataset\n",
    "        for batch in train_data:\n",
    "            # update progress bar\n",
    "            bar.update()\n",
    "\n",
    "            # split into data and labels. \n",
    "            data, labels = (\n",
    "                batch['image'].detach().clone().numpy(), \n",
    "                batch['label'].detach().clone().numpy()\n",
    "            )\n",
    "                \n",
    "            # compute loss\n",
    "            loss, grad, acc = model.loss(data, labels)\n",
    "\n",
    "            # collect historical data\n",
    "            history['train_loss'].append(loss)\n",
    "            history['train_acc'].append(acc)\n",
    "\n",
    "            # optimizer step\n",
    "            model.optim_step(grad)\n",
    "\n",
    "            # collect historical data\n",
    "            test_loss, test_acc = evaluate(model, test_data)\n",
    "            history['test_loss'].append(test_loss)\n",
    "            history['test_acc'].append(test_acc)\n",
    "            history['w'].append(model.w.copy())\n",
    "            \n",
    "            # update progress bar\n",
    "            bar.set_postfix({\n",
    "                'loss_trn':history['train_loss'][-1],\n",
    "                'loss_tst':history['test_loss'][-1],\n",
    "                'acc_trn':history['train_acc'][-1],\n",
    "                'acc_tst':history['test_acc'][-1],\n",
    "                'lr':model.lr_decayed,\n",
    "                '||w||':np.linalg.norm(history['w'][-1], ord=2),\n",
    "            })\n",
    "            \n",
    "\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6df1416a0747b480c0abbf47c3ba91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.01 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.08 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.06 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.03 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.05 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.01 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.04 seconds\n",
      "forward took 0.02 seconds\n",
      "forward took 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    lr=.01,\n",
    "    decay=.995,\n",
    "    reg=1,\n",
    "    num_epochs=40,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
